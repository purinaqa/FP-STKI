{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyiapkan Source Dataset\n",
    "Source Dataset akan digunakan untuk melatih CNN sebelum dilakukan proses Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/data_source/\"\n",
    "filename = \"islamicQA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kategori</th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jawaban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>terkait dengan penebus ghibah  apakah cukup de...</td>\n",
       "      <td>alhamdulillah        ghibah merupakan     dosa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>sampai kapan allah memaafkan hambanya yang ber...</td>\n",
       "      <td>alhamdulillah        allah ta ala berfirman   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>aku mengajukan hutang kepada salah satu bank s...</td>\n",
       "      <td>alhamdulillah        pertama         diharamka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>mengapa para shahabat yang dijamin masuk surga...</td>\n",
       "      <td>alhamdulillah     pertama      tidak diragukan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>apa tanda cinta allah kepada seorang hamba  da...</td>\n",
       "      <td>alhamdulillahsungguh anda telah bertanya denga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Kategori  \\\n",
       "0  adab  akhlak dan pensucian jiwa   \n",
       "1  adab  akhlak dan pensucian jiwa   \n",
       "2  adab  akhlak dan pensucian jiwa   \n",
       "3  adab  akhlak dan pensucian jiwa   \n",
       "4  adab  akhlak dan pensucian jiwa   \n",
       "\n",
       "                                          Pertanyaan  \\\n",
       "0  terkait dengan penebus ghibah  apakah cukup de...   \n",
       "1  sampai kapan allah memaafkan hambanya yang ber...   \n",
       "2  aku mengajukan hutang kepada salah satu bank s...   \n",
       "3  mengapa para shahabat yang dijamin masuk surga...   \n",
       "4  apa tanda cinta allah kepada seorang hamba  da...   \n",
       "\n",
       "                                             Jawaban  \n",
       "0  alhamdulillah        ghibah merupakan     dosa...  \n",
       "1  alhamdulillah        allah ta ala berfirman   ...  \n",
       "2  alhamdulillah        pertama         diharamka...  \n",
       "3  alhamdulillah     pertama      tidak diragukan...  \n",
       "4  alhamdulillahsungguh anda telah bertanya denga...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_path + filename, delimiter=',')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiqih dan usul fiqih                1043\n",
      "akidah                               234\n",
      "fiqih keluarga                       206\n",
      "adab  akhlak dan pensucian jiwa       64\n",
      "hadis dan ilmu ilmunya                64\n",
      "al qur an dan ilmu al qur an          50\n",
      "ilmu dan dakwah                       34\n",
      "politik islam                         15\n",
      "sejarah dan biografi                  14\n",
      "pendidikan                            13\n",
      "problematika kejiwaan dan sosial       6\n",
      "Name: Kategori, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kategori_total_list = train_df[\"Kategori\"].value_counts()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print (kategori_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kategori = train_df[\"Kategori\"].values\n",
    "tanya = train_df[\"Pertanyaan\"].values\n",
    "jawab = train_df[\"Jawaban\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 1743/1743 [15:26<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def getWordEmbedding(word, cursor):\n",
    "#     word = word.replace(\"'\", \"''\")\n",
    "    sql = \"\"\"select vec from term where term like %s\"\"\"\n",
    "    cursor.execute(sql, (str(word),))\n",
    "    data = cursor.fetchall()\n",
    "    if len(data) > 0:\n",
    "        decoded_vec = json.JSONDecoder().decode(data[0][0])\n",
    "        vec = np.asarray(decoded_vec, dtype=np.float32)\n",
    "        return True, vec\n",
    "    else:\n",
    "        return False, data\n",
    "    \n",
    "def myTokenizer(content, lower=True):\n",
    "    raw = content.split(' ')\n",
    "    remover = re.compile(\"[^a-zA-Z-]\")\n",
    "    \n",
    "    token = []\n",
    "    \n",
    "    for i in raw:\n",
    "        term = remover.sub('', i)\n",
    "        if lower == True:\n",
    "            term = term.lower()\n",
    "        token.append(term)\n",
    "#     tokenized = filter(None, token)\n",
    "    \n",
    "    return token\n",
    "\n",
    "def sentenceToVec(string):\n",
    "    if type(string) is not str:\n",
    "        return np.zeros((300,), dtype=float)\n",
    "    \n",
    "    string = string.replace('\\n', '')\n",
    "    string = np.array(myTokenizer(string))\n",
    "    \n",
    "    feature = None\n",
    "    begin = True\n",
    "    for word in string:\n",
    "        stat, vec = getWordEmbedding(word, cursor)\n",
    "        if not stat:\n",
    "            continue\n",
    "        if begin:\n",
    "            begin = False\n",
    "            feature = vec\n",
    "        else:\n",
    "            feature += vec\n",
    "            # feature = np.concatenate([feature, vec])\n",
    "    \n",
    "    if feature is not None:\n",
    "        feature = feature/np.linalg.norm(feature)\n",
    "    else:\n",
    "        feature = np.zeros((300,), dtype=float)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "db = mysql.connector.connect(user=\"root\", password='', database=\"glove\")\n",
    "cursor = db.cursor(buffered=True)\n",
    "\n",
    "pbar = tqdm(total=len(tanya))\n",
    "ftr = []\n",
    "for i in tanya:\n",
    "    try:\n",
    "        ftr.append(sentenceToVec(i))\n",
    "    except Exception as err:\n",
    "        print (err)\n",
    "        print (i)\n",
    "        break\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "ftr = np.array(ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06532822,  0.0588496 , -0.01162243, ...,  0.02337618,\n",
       "         0.07418327,  0.02370651],\n",
       "       [ 0.05333917,  0.03406325, -0.047201  , ...,  0.06848152,\n",
       "         0.02441938, -0.0102491 ],\n",
       "       [ 0.05779845,  0.02963272, -0.04286808, ...,  0.06795678,\n",
       "         0.06174847, -0.00555836],\n",
       "       ...,\n",
       "       [-0.0167215 ,  0.04160699, -0.03811455, ...,  0.05412887,\n",
       "         0.03542367,  0.02206324],\n",
       "       [ 0.05239877,  0.00257464, -0.05256515, ...,  0.05086377,\n",
       "         0.0189409 ,  0.03856589],\n",
       "       [ 0.06875054,  0.00971227, -0.05302511, ...,  0.06848324,\n",
       "         0.05816824,  0.03025898]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for i in range(len(kategori)):\n",
    "    if kategori[i] is np.nan:\n",
    "        nan_idx.append(i)\n",
    "kategori_c = np.delete(kategori, nan_idx)\n",
    "ftr_c = np.delete(ftr, nan_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06532822,  0.0588496 , -0.01162243, ...,  0.02337618,\n",
       "         0.07418327,  0.02370651],\n",
       "       [ 0.05333917,  0.03406325, -0.047201  , ...,  0.06848152,\n",
       "         0.02441938, -0.0102491 ],\n",
       "       [ 0.05779845,  0.02963272, -0.04286808, ...,  0.06795678,\n",
       "         0.06174847, -0.00555836],\n",
       "       ...,\n",
       "       [-0.0167215 ,  0.04160699, -0.03811455, ...,  0.05412887,\n",
       "         0.03542367,  0.02206324],\n",
       "       [ 0.05239877,  0.00257464, -0.05256515, ...,  0.05086377,\n",
       "         0.0189409 ,  0.03856589],\n",
       "       [ 0.06875054,  0.00971227, -0.05302511, ...,  0.06848324,\n",
       "         0.05816824,  0.03025898]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftr_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengecek array size dari fitur dan kelas...\n",
      "size of ftr (1743, 300)\n",
      "size of cls (1743, 11)\n"
     ]
    }
   ],
   "source": [
    "cls = []\n",
    "kategori_u = pd.unique(kategori_c)\n",
    "kategori_u = kategori_u.tolist()\n",
    "for i in range(len(kategori_c)):\n",
    "    one_hot = np.zeros((len(kategori_u),), dtype=int)\n",
    "    idx = kategori_u.index(kategori_c[i])\n",
    "    one_hot[idx] = 1\n",
    "    cls.append(one_hot)\n",
    "cls = np.array(cls)\n",
    "\n",
    "print (\"Mengecek array size dari fitur dan kelas...\")\n",
    "print (\"size of ftr \" + str(ftr_c.shape))\n",
    "print (\"size of cls \" + str(cls.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_path + \"ftr-data-source.npy\", ftr_c)\n",
    "np.save(data_path + \"cls-data-source.npy\", cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "Source Dataset ternyata tidak imbang antara satu kelas dan kelas lainnya. Maka, digunakan algoritma **SMOTE** untuk membuat jumlah data dan setiap kelasnya imbang (*balanced*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = np.load(data_path + \"ftr-data-source.npy\")\n",
    "cls = np.load(data_path + \"cls-data-source.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "indexed_cls = list(map(lambda x: np.argmax(x), cls))\n",
    "print(pd.unique(indexed_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuesCount(arr):\n",
    "    cls_dict = {}\n",
    "    for i in arr:\n",
    "        if i in cls_dict:\n",
    "            cls_dict[i] += 1\n",
    "        else:\n",
    "            cls_dict[i] = 1\n",
    "    return cls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 64,\n",
       " 1: 234,\n",
       " 2: 50,\n",
       " 3: 1043,\n",
       " 4: 206,\n",
       " 5: 64,\n",
       " 6: 34,\n",
       " 7: 13,\n",
       " 8: 15,\n",
       " 9: 6,\n",
       " 10: 14}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuesCount(indexed_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "ftr_res, cls_res = sm.fit_sample(ftr, indexed_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1043,\n",
       " 1: 1043,\n",
       " 2: 1043,\n",
       " 3: 1043,\n",
       " 4: 1043,\n",
       " 5: 1043,\n",
       " 6: 1043,\n",
       " 7: 1043,\n",
       " 8: 1043,\n",
       " 9: 1043,\n",
       " 10: 1043}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuesCount(cls_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convToOneHot(arr):\n",
    "    total_cls = len(pd.unique(arr))\n",
    "    cls = []\n",
    "    for i in arr:\n",
    "        temp_vec = np.zeros((total_cls,), dtype=int)\n",
    "        temp_vec[i] = 1\n",
    "        cls.append(temp_vec)\n",
    "    cls = np.array(cls)\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang array kelas seletah convToOneHot: 11\n"
     ]
    }
   ],
   "source": [
    "cls_fin = convToOneHot(cls_res)\n",
    "print (\"Panjang array kelas seletah convToOneHot: \" + str(len(cls_fin[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_path + \"ftr_data_source_balanced.npy\", ftr_res)\n",
    "np.save(data_path + \"cls_data_source_balanced.npy\", cls_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyiapkan Target Dataset\n",
    "Menyiapkan Target Dataset, mengubah bentuknya agar bisa menjadi masukan untuk CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target_path = \"data/data_target/\"\n",
    "filename_data_target = \"questions2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeInputMatrix(data, labels):\n",
    "    ftr_list = []\n",
    "    cls_list = []\n",
    "    \n",
    "    unique_labels = pd.unique(labels)\n",
    "    total_labels = len(unique_labels)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        cls = np.zeros((total_labels,), dtype=int)\n",
    "        cls_idx = np.where(unique_labels == labels[i])[0][0]\n",
    "        cls[cls_idx] = 1\n",
    "        \n",
    "        string = data[i].replace('\\n', '')\n",
    "        string = np.array(myTokenizer(string))\n",
    "\n",
    "        begin = True\n",
    "        for word in string:\n",
    "            stat, vec = getWordEmbedding(word, cursor)\n",
    "            if not stat:\n",
    "                continue\n",
    "            if begin:\n",
    "                begin = False\n",
    "                feature = vec\n",
    "            else:\n",
    "                feature += vec\n",
    "                # feature = np.concatenate([feature, vec])\n",
    "\n",
    "        feature = feature/np.linalg.norm(feature)\n",
    "        ftr_list.append(feature)\n",
    "        cls_list.append(cls)\n",
    "        \n",
    "    return np.array(ftr_list), np.array(cls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert into numpy array\n"
     ]
    }
   ],
   "source": [
    "print (\"convert into numpy array\")\n",
    "df = pd.read_csv(data_target_path + filename_data_target, sep=',', names=[\"pertanyaan\", \"kategori\"])\n",
    "tanya_target = df[\"pertanyaan\"].values\n",
    "label_target = df[\"kategori\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr, cls = arrangeInputMatrix(tanya_target, label_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking len of array\n",
      "True\n",
      "True\n",
      "checking size of arrays\n",
      "size of ftr: (481, 300)\n",
      "size pf cls: (481, 73)\n"
     ]
    }
   ],
   "source": [
    "print (\"checking len of array\")\n",
    "print (len(tanya_target) == len(ftr))\n",
    "print (len(tanya_target) == len(cls))\n",
    "\n",
    "print (\"checking size of arrays\")\n",
    "print (\"size of ftr: \" + str(ftr.shape))\n",
    "print (\"size pf cls: \" + str(cls.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_target_path + \"ftr_data_target.npy\", ftr)\n",
    "np.save(data_target_path + \"cls_data_target.npy\", cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "Memisahkan Source Dataset dan Target Dataset menjadi Train Set dan Test Tes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_data_target = np.load(data_target_path + \"ftr_data_target.npy\")\n",
    "cls_data_target = np.load(data_target_path + \"cls_data_target.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(481, 300)\n",
      "(481, 73)\n"
     ]
    }
   ],
   "source": [
    "print (ftr_data_target.shape)\n",
    "print(cls_data_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target_path_splitted = \"data/splitted/data_target/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=0)\n",
    "# for train_index, test_index in sss.split(ftr_data_target, cls_data_target):\n",
    "#     X, X_test = ftr_data_target[train_index], ftr_data_target[test_index]\n",
    "#     Y, Y_test = cls_data_target[train_index], cls_data_target[test_index]\n",
    "#     break\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(ftr_data_target, cls_data_target, test_size=0.3, random_state=42) #stratify agar pembagian tiap kelas merata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_target_path_splitted + \"X.npy\", X)\n",
    "np.save(data_target_path_splitted + \"Y.npy\", Y)\n",
    "np.save(data_target_path_splitted + \"X_test.npy\", X_test)\n",
    "np.save(data_target_path_splitted + \"Y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Source Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path = \"data/data_source/\"\n",
    "\n",
    "ftr_data_source = np.load(data_source_path + \"ftr_data_source_balanced.npy\")\n",
    "cls_data_source = np.load(data_source_path + \"cls_data_source_balanced.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path_splitted = \"data/splitted/data_source/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(ftr_data_source, cls_data_source):\n",
    "    X, X_test = ftr_data_source[train_index], ftr_data_source[test_index]\n",
    "    Y, Y_test = cls_data_source[train_index], cls_data_source[test_index]\n",
    "    break\n",
    "# X = np.expand_dims(X, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_source_path_splitted + \"X.npy\", X)\n",
    "np.save(data_source_path_splitted + \"Y.npy\", Y)\n",
    "np.save(data_source_path_splitted + \"X_test.npy\", X_test)\n",
    "np.save(data_source_path_splitted + \"Y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coba pake RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# x = np.load(\"data/data_target/ftr_data_target.npy\")\n",
    "# y = np.load(\"data/data_target/cls_data_target.npy\")\n",
    "X_train = np.load(\"data/splitted/data_target/X.npy\")\n",
    "y_train = np.load(\"data/splitted/data_target/Y.npy\")\n",
    "X_test = np.load(\"data/splitted/data_target/X_test.npy\")\n",
    "y_test = np.load(\"data/splitted/data_target/Y_test.npy\")\n",
    "# X_train = np.load(\"data/splitted/data_source/X.npy\")\n",
    "# y_train = np.load(\"data/splitted/data_source/Y.npy\")\n",
    "# X_test = np.load(\"data/splitted/data_source/X_test.npy\")\n",
    "# y_test = np.load(\"data/splitted/data_source/Y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\purina qa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0896551724137931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\purina qa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\purina qa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.19310344827586207, 0.0896551724137931, 0.11862068965517239, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "RandomForest = RandomForestClassifier()\n",
    "RandomForest.fit(X_train, y_train)\n",
    "y_RF = RandomForest.predict(X_test)\n",
    "print(accuracy_score(y_test, y_RF))\n",
    "print(precision_recall_fscore_support(y_test, y_RF, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
