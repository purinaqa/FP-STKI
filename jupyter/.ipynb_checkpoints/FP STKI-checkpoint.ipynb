{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyiapkan Source Dataset\n",
    "Source Dataset akan digunakan untuk melatih CNN sebelum dilakukan proses Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/data_source/\"\n",
    "filename = \"islamicQA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kategori</th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jawaban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>terkait dengan penebus ghibah  apakah cukup de...</td>\n",
       "      <td>alhamdulillah        ghibah merupakan     dosa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>sampai kapan allah memaafkan hambanya yang ber...</td>\n",
       "      <td>alhamdulillah        allah ta ala berfirman   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>aku mengajukan hutang kepada salah satu bank s...</td>\n",
       "      <td>alhamdulillah        pertama         diharamka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>mengapa para shahabat yang dijamin masuk surga...</td>\n",
       "      <td>alhamdulillah     pertama      tidak diragukan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adab  akhlak dan pensucian jiwa</td>\n",
       "      <td>apa tanda cinta allah kepada seorang hamba  da...</td>\n",
       "      <td>alhamdulillahsungguh anda telah bertanya denga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Kategori  \\\n",
       "0  adab  akhlak dan pensucian jiwa   \n",
       "1  adab  akhlak dan pensucian jiwa   \n",
       "2  adab  akhlak dan pensucian jiwa   \n",
       "3  adab  akhlak dan pensucian jiwa   \n",
       "4  adab  akhlak dan pensucian jiwa   \n",
       "\n",
       "                                          Pertanyaan  \\\n",
       "0  terkait dengan penebus ghibah  apakah cukup de...   \n",
       "1  sampai kapan allah memaafkan hambanya yang ber...   \n",
       "2  aku mengajukan hutang kepada salah satu bank s...   \n",
       "3  mengapa para shahabat yang dijamin masuk surga...   \n",
       "4  apa tanda cinta allah kepada seorang hamba  da...   \n",
       "\n",
       "                                             Jawaban  \n",
       "0  alhamdulillah        ghibah merupakan     dosa...  \n",
       "1  alhamdulillah        allah ta ala berfirman   ...  \n",
       "2  alhamdulillah        pertama         diharamka...  \n",
       "3  alhamdulillah     pertama      tidak diragukan...  \n",
       "4  alhamdulillahsungguh anda telah bertanya denga...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_path + filename, delimiter=',')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiqih dan usul fiqih                1043\n",
      "akidah                               234\n",
      "fiqih keluarga                       206\n",
      "hadis dan ilmu ilmunya                64\n",
      "adab  akhlak dan pensucian jiwa       64\n",
      "al qur an dan ilmu al qur an          50\n",
      "ilmu dan dakwah                       34\n",
      "politik islam                         15\n",
      "sejarah dan biografi                  14\n",
      "pendidikan                            13\n",
      "problematika kejiwaan dan sosial       6\n",
      "Name: Kategori, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kategori_total_list = train_df[\"Kategori\"].value_counts()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print (kategori_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori = train_df[\"Kategori\"].values\n",
    "for i in range(len(kategori)):\n",
    "    kat = str(kategori[i]).lower()\n",
    "    if \"sehat\" in kat or \"obat\" in kat or \"jamkesmas\" in kat:\n",
    "        kategori[i] = \"Kesehatan\"\n",
    "    elif \"banjir\" in kat or \"bencana\" in kat:\n",
    "        kategori[i] = \"Lingkungan Hidup dan Penanggulangan Bencana\"\n",
    "    elif \"bbm\" in kat:\n",
    "        kategori[i] = \"BBM\"\n",
    "    elif \"perekonomian\" in kat:\n",
    "        kategori[i] = \"Perekonomian\"\n",
    "    elif \"usaha\" in kat:\n",
    "        kategori[i] = \"Perdagangan, Perindustrian, Iklim Usaha, dan Investasi\"\n",
    "    elif \"pendidikan\" in kat:\n",
    "        kategori[i] = \"Pendidikan\"\n",
    "    elif \"kewaspadaan\" in kat or \"perundungan\" in kat or \"kantor cabang\" in kat or \"sms\" in kat or \"haji\" in kat or \"pemberdayaan masyarakat\" in kat or \"situasi khusus\" in kat:\n",
    "        kategori[i] = \"Topik Lainnya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Kategori Edited\"] = kategori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bantuan Langsung Sementara Masyarakat (BLSM)                                14739\n",
      "Topik Lainnya                                                                9218\n",
      "Infrastruktur                                                                6949\n",
      "Reformasi Birokrasi dan Tata Kelola                                          6840\n",
      "Bantuan Siswa Miskin (BSM)                                                   6608\n",
      "Beras Miskin (Raskin)                                                        5952\n",
      "Kepesertaan Non-KPS                                                          4557\n",
      "Pendidikan                                                                   3892\n",
      "Kesehatan                                                                    3615\n",
      "Lingkungan Hidup dan Penanggulangan Bencana                                  3129\n",
      "Kartu Indonesia Pintar (KIP)                                                 2391\n",
      "Kepesertaan KPS                                                              2036\n",
      "BBM                                                                          2029\n",
      "Perhubungan                                                                  1134\n",
      "Bidang Kesejahteraan Rakyat                                                   980\n",
      "Bidang Politik, Hukum, dan Keamanan                                           876\n",
      "Administrasi Kependudukan                                                     860\n",
      "Layanan Keuangan Digital (LKD)                                                743\n",
      "Energi dan Sumber Daya Alam                                                   720\n",
      "Pertanahan dan Permukiman                                                     642\n",
      "Pelayanan Administrasi                                                        593\n",
      "Kepesertaan Baru                                                              589\n",
      "Kepesertaan - Kartu & Non Kartu                                               553\n",
      "Perdagangan, Perindustrian, Iklim Usaha, dan Investasi                        545\n",
      "Teknologi Informasi dan Komunikasi                                            428\n",
      "Lingkungan Hidup                                                              420\n",
      "Imigrasi                                                                      399\n",
      "Listrik                                                                       399\n",
      "Jadwal / Waktu                                                                370\n",
      "Kepegawaian                                                                   363\n",
      "Ketenagakerjaan                                                               350\n",
      "Penyelewengan                                                                 342\n",
      "Kepolisian                                                                    340\n",
      "Pelayanan Masyarakat                                                          324\n",
      "Program Keluarga Harapan (PKH)                                                317\n",
      "Pengentasan Kemiskinan                                                        305\n",
      "Keamanan dan Ketertiban Masyarakat                                            290\n",
      "Administrasi pendaftaran online                                               220\n",
      "Perekonomian                                                                  220\n",
      "Pertanian                                                                     218\n",
      "Layanan Pos                                                                   195\n",
      "Informasi Umum                                                                180\n",
      "Layanan Air Minum                                                             177\n",
      "Pajak                                                                         165\n",
      "Pemasyarakatan                                                                148\n",
      "Pembangunan Desa, Daerah Tertinggal, Terdepan, Terluar, dan Transmigrasi      120\n",
      "Iuran                                                                         114\n",
      "Kritik/Saran untuk LAPOR!                                                     104\n",
      "Permasalahan                                                                  100\n",
      "Migas                                                                         100\n",
      "Topik Khusus                                                                   73\n",
      "Koperasi, UKM, dan Ekonomi Kreatif                                             65\n",
      "Penyalahgunaan Wewenang                                                        61\n",
      "Bea dan Cukai                                                                  53\n",
      "Perbankan                                                                      51\n",
      "Kompensasi Lainnya                                                             47\n",
      "Tagihan iuran tidak sesuai                                                     46\n",
      "Polhukam                                                                       41\n",
      "Korupsi                                                                        40\n",
      "Kesulitan pembayaran iuran                                                     37\n",
      "Kebudayaan dan Pariwisata                                                      36\n",
      "Penyerapan Anggaran                                                            36\n",
      "Kemaritiman                                                                    27\n",
      "Kartu Keluarga Sejahtera (KKS)                                                 25\n",
      "Kehutanan                                                                      22\n",
      "Status Kepesertaan tidak aktif                                                 16\n",
      "Pertambangan                                                                   16\n",
      "Kesalahan pembayaran iuran                                                     12\n",
      "Name: Kategori, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kategori_total_list = train_df[\"Kategori\"].value_counts()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print (kategori_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pak Menteri Kesehatan. Tolong perhatikan kami RSU di daerah NTT, tenaga Dokter kurang & pelayanan tidak maksimal, tapi bisa melayani 3 s/d 4 tempat. Sangat berbahaya... Trims.',\n",
       "       'Kemenkumham, standarisasi minimal pornografi & pornoaksi tolong dibuat dan disosialisasikan kepada masyarakat.',\n",
       "       'Menkes RI yang terhormat, tolong inspeksi ke RSUD WZ. Yohanis, Kupang. Jika pasien tidak mampu, operasi penyakit dalam diminta bayar sekitar 10 juta, meskipun ada keterangan tidak mampu. Apakah ini benar?',\n",
       "       ...,\n",
       "       'Lstrik di ribuan pelanggan PLN Desa Brabe Kec. Maron Kab. Probolinggo, Jatim, Wilayah Kerja PLN Kraksaan Kab. Prob Jatim sering padam berjam-jam, antara lain tgl 15,24,25 September, 5,13 Oktober,9,10,11,13,16,30 November,1 Desember padam dan saat ini 3 De',\n",
       "       'Kepada yth pengurus BPJSTK, saya mohon bantuannya, waktu saya daftar pertama kali menggunakan nmr HP 089601729737, namun sekarang no tsb b sudah tidak aktip, ketika saya ingin cek saldo bpjs di hp jadi kesulitan dan tidak bisa menggunakan no hp yg baru...',\n",
       "       'Pada hari jumat tanggal 2 Oktober 2015 saya mendaftarkan pemecahan atau pemisahan sertipikat tanah di kantor BPN Jakarta Barat dengan nomer berkas 058809/2015 dan terakhir saya cek ke kantor BPN Jakarta Barat hari Jumat tanggal 27 November 2015 posisi ber'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laporan = train_df[\"IsiLaporan\"].values\n",
    "laporan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87766/87766 [1:50:34<00:00, 13.23it/s]  \n"
     ]
    }
   ],
   "source": [
    "def getWordEmbedding(word, cursor):\n",
    "#     word = word.replace(\"'\", \"''\")\n",
    "    sql = \"\"\"select vec from term where term like %s\"\"\"\n",
    "    cursor.execute(sql, (str(word),))\n",
    "    data = cursor.fetchall()\n",
    "    if len(data) > 0:\n",
    "        decoded_vec = json.JSONDecoder().decode(data[0][0])\n",
    "        vec = np.asarray(decoded_vec, dtype=np.float32)\n",
    "        return True, vec\n",
    "    else:\n",
    "        return False, data\n",
    "    \n",
    "def myTokenizer(content, lower=True):\n",
    "    raw = content.split(' ')\n",
    "    remover = re.compile(\"[^a-zA-Z-]\")\n",
    "    \n",
    "    token = []\n",
    "    \n",
    "    for i in raw:\n",
    "        term = remover.sub('', i)\n",
    "        if lower == True:\n",
    "            term = term.lower()\n",
    "        token.append(term)\n",
    "    tokenized = filter(None, token)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "def sentenceToVec(string):\n",
    "    if type(string) is not str:\n",
    "        return np.zeros((300,), dtype=float)\n",
    "    \n",
    "    string = string.replace('\\n', '')\n",
    "    string = np.array(myTokenizer(string))\n",
    "    \n",
    "    feature = None\n",
    "    begin = True\n",
    "    for word in string:\n",
    "        stat, vec = getWordEmbedding(word, cursor)\n",
    "        if not stat:\n",
    "            continue\n",
    "        if begin:\n",
    "            begin = False\n",
    "            feature = vec\n",
    "        else:\n",
    "            feature += vec\n",
    "            # feature = np.concatenate([feature, vec])\n",
    "    \n",
    "    if feature is not None:\n",
    "        feature = feature/np.linalg.norm(feature)\n",
    "    else:\n",
    "        feature = np.zeros((300,), dtype=float)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "db = mysql.connector.connect(user=\"root\", password='', database=\"glove-300\")\n",
    "cursor = db.cursor(buffered=True)\n",
    "\n",
    "pbar = tqdm(total=len(laporan))\n",
    "ftr = []\n",
    "for i in laporan:\n",
    "    try:\n",
    "        ftr.append(sentenceToVec(i))\n",
    "    except Exception as err:\n",
    "        print err\n",
    "        print i\n",
    "        break\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "ftr = np.array(ftr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_idx = []\n",
    "for i in range(len(kategori)):\n",
    "    if kategori[i] is np.nan:\n",
    "        nan_idx.append(i)\n",
    "kategori_c = np.delete(kategori, nan_idx)\n",
    "ftr_c = np.delete(ftr, nan_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengecek array size dari fitur dan kelas...\n",
      "size of ftr (87602L, 300L)\n",
      "size of cls (87602L, 68L)\n"
     ]
    }
   ],
   "source": [
    "cls = []\n",
    "kategori_u = pd.unique(kategori_c)\n",
    "kategori_u = kategori_u.tolist()\n",
    "for i in range(len(kategori_c)):\n",
    "    one_hot = np.zeros((len(kategori_u),), dtype=int)\n",
    "    idx = kategori_u.index(kategori_c[i])\n",
    "    one_hot[idx] = 1\n",
    "    cls.append(one_hot)\n",
    "cls = np.array(cls)\n",
    "\n",
    "print \"Mengecek array size dari fitur dan kelas...\"\n",
    "print \"size of ftr \" + str(ftr_c.shape)\n",
    "print \"size of cls \" + str(cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_path + \"ftr-data-source.npy\", ftr_c)\n",
    "np.save(data_path + \"cls-data-source.npy\", cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "Source Dataset ternyata tidak imbang antara satu kelas dan kelas lainnya. Maka, digunakan algoritma **SMOTE** untuk membuat jumlah data dan setiap kelasnya imbang (*balanced*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = np.load(data_path + \"ftr-data-source.npy\")\n",
    "cls = np.load(data_path + \"cls-data-source.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_cls = map(lambda x: np.argmax(x), cls)\n",
    "pd.unique(indexed_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuesCount(arr):\n",
    "    cls_dict = {}\n",
    "    for i in arr:\n",
    "        if i in cls_dict:\n",
    "            cls_dict[i] += 1\n",
    "        else:\n",
    "            cls_dict[i] = 1\n",
    "    return cls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 980,\n",
       " 1: 428,\n",
       " 2: 3615,\n",
       " 3: 3129,\n",
       " 4: 3892,\n",
       " 5: 6840,\n",
       " 6: 545,\n",
       " 7: 218,\n",
       " 8: 876,\n",
       " 9: 720,\n",
       " 10: 73,\n",
       " 11: 305,\n",
       " 12: 9218,\n",
       " 13: 220,\n",
       " 14: 40,\n",
       " 15: 6949,\n",
       " 16: 120,\n",
       " 17: 36,\n",
       " 18: 1134,\n",
       " 19: 350,\n",
       " 20: 642,\n",
       " 21: 5952,\n",
       " 22: 2029,\n",
       " 23: 4557,\n",
       " 24: 14739,\n",
       " 25: 2036,\n",
       " 26: 6608,\n",
       " 27: 47,\n",
       " 28: 317,\n",
       " 29: 290,\n",
       " 30: 399,\n",
       " 31: 420,\n",
       " 32: 363,\n",
       " 33: 593,\n",
       " 34: 100,\n",
       " 35: 860,\n",
       " 36: 114,\n",
       " 37: 27,\n",
       " 38: 53,\n",
       " 39: 2391,\n",
       " 40: 589,\n",
       " 41: 370,\n",
       " 42: 553,\n",
       " 43: 340,\n",
       " 44: 743,\n",
       " 45: 342,\n",
       " 46: 180,\n",
       " 47: 100,\n",
       " 48: 25,\n",
       " 49: 16,\n",
       " 50: 195,\n",
       " 51: 41,\n",
       " 52: 165,\n",
       " 53: 16,\n",
       " 54: 148,\n",
       " 55: 51,\n",
       " 56: 399,\n",
       " 57: 177,\n",
       " 58: 22,\n",
       " 59: 37,\n",
       " 60: 36,\n",
       " 61: 220,\n",
       " 62: 46,\n",
       " 63: 104,\n",
       " 64: 12,\n",
       " 65: 61,\n",
       " 66: 65,\n",
       " 67: 324}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuesCount(indexed_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "ftr_res, cls_res = sm.fit_sample(ftr, indexed_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 14739,\n",
       " 1: 14739,\n",
       " 2: 14739,\n",
       " 3: 14739,\n",
       " 4: 14739,\n",
       " 5: 14739,\n",
       " 6: 14739,\n",
       " 7: 14739,\n",
       " 8: 14739,\n",
       " 9: 14739,\n",
       " 10: 14739,\n",
       " 11: 14739,\n",
       " 12: 14739,\n",
       " 13: 14739,\n",
       " 14: 14739,\n",
       " 15: 14739,\n",
       " 16: 14739,\n",
       " 17: 14739,\n",
       " 18: 14739,\n",
       " 19: 14739,\n",
       " 20: 14739,\n",
       " 21: 14739,\n",
       " 22: 14739,\n",
       " 23: 14739,\n",
       " 24: 14739,\n",
       " 25: 14739,\n",
       " 26: 14739,\n",
       " 27: 14739,\n",
       " 28: 14739,\n",
       " 29: 14739,\n",
       " 30: 14739,\n",
       " 31: 14739,\n",
       " 32: 14739,\n",
       " 33: 14739,\n",
       " 34: 14739,\n",
       " 35: 14739,\n",
       " 36: 14739,\n",
       " 37: 14739,\n",
       " 38: 14739,\n",
       " 39: 14739,\n",
       " 40: 14739,\n",
       " 41: 14739,\n",
       " 42: 14739,\n",
       " 43: 14739,\n",
       " 44: 14739,\n",
       " 45: 14739,\n",
       " 46: 14739,\n",
       " 47: 14739,\n",
       " 48: 14739,\n",
       " 49: 14739,\n",
       " 50: 14739,\n",
       " 51: 14739,\n",
       " 52: 14739,\n",
       " 53: 14739,\n",
       " 54: 14739,\n",
       " 55: 14739,\n",
       " 56: 14739,\n",
       " 57: 14739,\n",
       " 58: 14739,\n",
       " 59: 14739,\n",
       " 60: 14739,\n",
       " 61: 14739,\n",
       " 62: 14739,\n",
       " 63: 14739,\n",
       " 64: 14739,\n",
       " 65: 14739,\n",
       " 66: 14739,\n",
       " 67: 14739}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valuesCount(cls_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convToOneHot(arr):\n",
    "    total_cls = len(pd.unique(arr))\n",
    "    cls = []\n",
    "    for i in arr:\n",
    "        temp_vec = np.zeros((total_cls,), dtype=int)\n",
    "        temp_vec[i] = 1\n",
    "        cls.append(temp_vec)\n",
    "    cls = np.array(cls)\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang array kelas seletah convToOneHot: 68\n"
     ]
    }
   ],
   "source": [
    "cls_fin = convToOneHot(cls_res)\n",
    "print \"Panjang array kelas seletah convToOneHot: \" + str(len(cls_fin[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(data_path + \"ftr_data_source_balanced.npy\", ftr_res)\n",
    "np.save(data_path + \"cls_data_source_balanced.npy\", cls_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyiapkan Target Dataset\n",
    "Menyiapkan Target Dataset, mengubah bentuknya agar bisa menjadi masukan untuk CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target_path = \"data/data_target/\"\n",
    "filename_data_target = \"data_islamicQA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeInputMatrix(data, labels):\n",
    "    ftr_list = []\n",
    "    cls_list = []\n",
    "    \n",
    "    unique_labels = pd.unique(labels)\n",
    "    total_labels = len(unique_labels)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        cls = np.zeros((total_labels,), dtype=int)\n",
    "        cls_idx = np.where(unique_labels == labels[i])[0][0]\n",
    "        cls[cls_idx] = 1\n",
    "        \n",
    "        string = data[i].replace('\\n', '')\n",
    "        string = np.array(myTokenizer(string))\n",
    "\n",
    "        begin = True\n",
    "        for word in string:\n",
    "            stat, vec = getWordEmbedding(word, cursor)\n",
    "            if not stat:\n",
    "                continue\n",
    "            if begin:\n",
    "                begin = False\n",
    "                feature = vec\n",
    "            else:\n",
    "                feature += vec\n",
    "                # feature = np.concatenate([feature, vec])\n",
    "\n",
    "        feature = feature/np.linalg.norm(feature)\n",
    "        ftr_list.append(feature)\n",
    "        cls_list.append(cls)\n",
    "        \n",
    "    return np.array(ftr_list), np.array(cls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert into numpy array\n"
     ]
    }
   ],
   "source": [
    "print \"convert into numpy array\"\n",
    "df = pd.read_csv(data_target_path + filename_data_target, sep=',', names=[\"documents\", \"labels\"])\n",
    "documents = df[\"documents\"].values\n",
    "labels = df[\"labels\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-00297ed73263>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mftr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrangeInputMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-f2700f38cb5e>\u001b[0m in \u001b[0;36marrangeInputMatrix\u001b[1;34m(data, labels)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mcls_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_labels\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "ftr, cls = arrangeInputMatrix(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking len of array\n",
      "True\n",
      "True\n",
      "\n",
      "checking size of arrays\n",
      "size of ftr: (462L, 300L)\n",
      "size pf cls: (462L, 22L)\n"
     ]
    }
   ],
   "source": [
    "print \"checking len of array\"\n",
    "print len(documents) == len(ftr)\n",
    "print len(documents) == len(cls)\n",
    "\n",
    "print\n",
    "print \"checking size of arrays\"\n",
    "print \"size of ftr: \" + str(ftr.shape)\n",
    "print \"size pf cls: \" + str(cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_target_path + \"ftr_data_target.npy\", ftr)\n",
    "np.save(data_target_path + \"cls_data_target.npy\", cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "Memisahkan Source Dataset dan Target Dataset menjadi Train Set dan Test Tes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr_data_target = np.load(data_target_path + \"ftr_data_target.npy\")\n",
    "cls_data_target = np.load(data_target_path + \"cls_data_target.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target_path_splitted = \"data/splitted/data_target/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=0)\n",
    "for train_index, test_index in sss.split(ftr_data_target, cls_data_target):\n",
    "    X, X_test = ftr_data_target[train_index], ftr_data_target[test_index]\n",
    "    Y, Y_test = cls_data_target[train_index], cls_data_target[test_index]\n",
    "    break\n",
    "# X = np.expand_dims(X, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_target_path_splitted + \"X.npy\", X)\n",
    "np.save(data_target_path_splitted + \"Y.npy\", Y)\n",
    "np.save(data_target_path_splitted + \"X_test.npy\", X_test)\n",
    "np.save(data_target_path_splitted + \"Y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Source Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path = \"data/data_source/\"\n",
    "\n",
    "ftr_data_source = np.load(data_source_path + \"ftr_data_source_balanced.npy\")\n",
    "cls_data_source = np.load(data_source_path + \"cls_data_source_balanced.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_path_splitted = \"data/splitted/data_source/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(ftr_data_source, cls_data_source):\n",
    "    X, X_test = ftr_data_source[train_index], ftr_data_source[test_index]\n",
    "    Y, Y_test = cls_data_source[train_index], cls_data_source[test_index]\n",
    "    break\n",
    "# X = np.expand_dims(X, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_source_path_splitted + \"X.npy\", X)\n",
    "np.save(data_source_path_splitted + \"Y.npy\", Y)\n",
    "np.save(data_source_path_splitted + \"X_test.npy\", X_test)\n",
    "np.save(data_source_path_splitted + \"Y_test.npy\", Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
